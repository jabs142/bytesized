# Data Flow: Where Comments Are Saved

## The Complete Journey of Your Data

```
Reddit API
    ‚Üì
[1] reddit_collector.py collects data
    ‚Üì
[2] _extract_post_data() structures it
    ‚Üì
[3] _extract_top_comments() gets comments
    ‚Üì
[4] save_posts() writes JSON file
    ‚Üì
data/raw/reddit_mental_health_posts_TIMESTAMP.json
    ‚Üì
[Later] Loaded by Jupyter notebook for analysis
```

---

## Storage Details

### **Where are comments saved?**

**Location**: `data/raw/reddit_mental_health_posts_YYYYMMDD_HHMMSS.json`

**Format**: Raw JSON (no processing yet)

**Structure**:
```json
[
  {
    "id": "abc123",
    "title": "Depression from IUD",
    "selftext": "I've been feeling terrible...",
    "score": 45,
    "num_comments": 12,
    "top_comments": [                    ‚Üê HERE!
      {
        "id": "comment1",
        "text": "I experienced the same thing...",
        "score": 15,
        "created_utc": 1234567890
      },
      {
        "id": "comment2",
        "text": "Have you talked to your doctor?",
        "score": 8,
        "created_utc": 1234567891
      }
    ]
  },
  {
    "id": "def456",
    "title": "Another post...",
    "top_comments": [...]
  }
]
```

---

## Raw vs. Processed

### **Currently: RAW DATA** ü•©

Comments are saved **exactly as extracted** from Reddit:
- ‚úÖ Original text (no cleaning)
- ‚úÖ Original score
- ‚úÖ Timestamp
- ‚ùå NOT cleaned (still has URLs, special chars)
- ‚ùå NOT filtered (all scores included)
- ‚ùå NOT analyzed (no sentiment, entities extracted)

### **Later: PROCESSED DATA** üç≥

After preprocessing (`text_cleaner.py`), we'll create:

**Location**: `data/processed/cleaned_posts.json`

**Changes**:
```json
{
  "id": "abc123",
  "title": "Depression from IUD",
  "selftext": "I've been feeling terrible",  ‚Üê Cleaned!
  "top_comments": [
    {
      "text": "I experienced the same thing",  ‚Üê PII removed
      "score": 15,
      "cleaned": true  ‚Üê Metadata added
    }
  ]
}
```

---

## Why Keep Raw + Processed Separate?

### **Raw data** (`data/raw/`):
- ‚úÖ Original source of truth
- ‚úÖ Can re-process differently later
- ‚úÖ Backup if processing goes wrong

### **Processed data** (`data/processed/`):
- ‚úÖ Ready for analysis
- ‚úÖ Privacy-safe (PII removed)
- ‚úÖ Cleaner, faster to work with

**Best practice**: NEVER delete raw data, always keep it!

---

## Example: Full Data Lifecycle

```python
# Step 1: Collection (raw)
collector.search_subreddit('birthcontrol', ['depression'])
# Saves: data/raw/reddit_mental_health_posts_20231027_143000.json

# Step 2: Cleaning (processed)
cleaner = TextCleaner()
cleaned = cleaner.clean_dataset(raw_posts)
# Saves: data/processed/cleaned_posts_20231027.json

# Step 3: Analysis (interim)
# Extract entities, sentiment, etc.
# Saves: data/interim/extracted_entities.json

# Step 4: Final output (outputs/)
# Visualizations, reports, insights
# Saves: outputs/reports/analysis_summary.pdf
```

---

## How to Find Your Saved Data

```bash
# List all collected data files
ls -lh data/raw/

# View the most recent file
cat data/raw/reddit_mental_health_posts_*.json | head -100

# Count posts in a file
python -c "import json; print(len(json.load(open('data/raw/reddit_mental_health_posts_20231027_143000.json'))))"

# Check if comments are included
python -c "import json; data = json.load(open('data/raw/reddit_mental_health_posts_20231027_143000.json')); print('Has comments:', 'top_comments' in data[0])"
```
