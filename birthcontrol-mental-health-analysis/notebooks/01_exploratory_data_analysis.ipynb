{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis: Birth Control Mental Health Posts\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the structure of collected Reddit data\n",
    "- Perform basic statistical analysis\n",
    "- Visualize text data distributions\n",
    "- Identify patterns and prepare for NLP/LLM tasks\n",
    "\n",
    "## What is EDA?\n",
    "Exploratory Data Analysis (EDA) is the critical first step in any ML/AI project:\n",
    "- **Understand your data**: What do you actually have?\n",
    "- **Identify quality issues**: Missing data, duplicates, errors\n",
    "- **Find patterns**: What insights emerge before modeling?\n",
    "- **Guide decisions**: What preprocessing/modeling approach makes sense?\n",
    "\n",
    "Rule of thumb: Spend 80% of time on data quality, 20% on modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Data\n",
    "\n",
    "First, let's load our collected Reddit posts and examine the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most recent data file\n",
    "data_dir = Path('../data/raw')\n",
    "data_files = list(data_dir.glob('reddit_mental_health_posts_*.json'))\n",
    "\n",
    "if not data_files:\n",
    "    print(\"âš ï¸  No data files found!\")\n",
    "    print(\"   Please run: python src/data_collection/reddit_collector.py\")\n",
    "else:\n",
    "    # Use the most recent file\n",
    "    latest_file = max(data_files, key=lambda p: p.stat().st_mtime)\n",
    "    print(f\"ðŸ“‚ Loading: {latest_file.name}\")\n",
    "    \n",
    "    # Load JSON data\n",
    "    with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "        posts_raw = json.load(f)\n",
    "    \n",
    "    # Convert to pandas DataFrame for easier analysis\n",
    "    # LEARNING: DataFrames are the standard for tabular data in Python\n",
    "    df = pd.DataFrame(posts_raw)\n",
    "    \n",
    "    print(f\"âœ“ Loaded {len(df)} posts\")\n",
    "    print(f\"\\nDataFrame shape: {df.shape[0]} rows Ã— {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine first few rows\n",
    "# LEARNING: Always inspect raw data before analysis\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and memory usage\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Checks\n",
    "\n",
    "Before any analysis, we need to check data quality:\n",
    "- Missing values\n",
    "- Duplicates\n",
    "- Data consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(\"=\" * 40)\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "# INSIGHT: Empty selftext is common for link posts\n",
    "# We'll filter for text posts later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df.duplicated(subset=['id']).sum()\n",
    "print(f\"\\nDuplicate posts: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"Removing duplicates...\")\n",
    "    df = df.drop_duplicates(subset=['id'], keep='first')\n",
    "    print(f\"âœ“ After deduplication: {len(df)} posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for text posts only (where we have content to analyze)\n",
    "text_posts = df[df['is_self'] == True].copy()\n",
    "text_posts = text_posts[text_posts['text_length'] > 0]\n",
    "\n",
    "print(f\"Text posts with content: {len(text_posts)} / {len(df)}\")\n",
    "print(f\"Percentage: {len(text_posts)/len(df)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distribution Analysis\n",
    "\n",
    "Understanding the distribution of our data helps us:\n",
    "- Identify outliers\n",
    "- Understand typical post characteristics\n",
    "- Plan preprocessing strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subreddit distribution\n",
    "subreddit_counts = text_posts['subreddit'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "subreddit_counts.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Posts by Subreddit', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Subreddit')\n",
    "ax.set_ylabel('Number of Posts')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPosts per subreddit:\")\n",
    "print(subreddit_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distribution\n",
    "# LEARNING: Text length affects:\n",
    "# - LLM token usage (and cost!)\n",
    "# - Processing time\n",
    "# - Information density\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(text_posts['text_length'], bins=50, edgecolor='black')\n",
    "axes[0].set_title('Distribution of Post Lengths', fontweight='bold')\n",
    "axes[0].set_xlabel('Text Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(text_posts['text_length'].median(), color='red', \n",
    "                linestyle='--', label=f'Median: {text_posts[\"text_length\"].median():.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(text_posts['text_length'])\n",
    "axes[1].set_title('Post Length Box Plot', fontweight='bold')\n",
    "axes[1].set_ylabel('Text Length (characters)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nText Length Statistics:\")\n",
    "print(text_posts['text_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post score (engagement) distribution\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.hist(text_posts['score'], bins=50, edgecolor='black')\n",
    "ax.set_title('Distribution of Post Scores (Engagement)', fontweight='bold')\n",
    "ax.set_xlabel('Score (upvotes - downvotes)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.axvline(text_posts['score'].median(), color='red', \n",
    "           linestyle='--', label=f'Median: {text_posts[\"score\"].median():.0f}')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nScore Statistics:\")\n",
    "print(text_posts['score'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temporal Analysis\n",
    "\n",
    "When were these posts made? Are there patterns over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime\n",
    "text_posts['created_datetime'] = pd.to_datetime(text_posts['created_date'])\n",
    "text_posts['year_month'] = text_posts['created_datetime'].dt.to_period('M')\n",
    "\n",
    "# Posts over time\n",
    "temporal_dist = text_posts.groupby('year_month').size()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "temporal_dist.plot(kind='line', marker='o', ax=ax)\n",
    "ax.set_title('Posts Over Time', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Number of Posts')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Content Analysis\n",
    "\n",
    "Now let's analyze the actual text content - this is where NLP begins!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and text for full content analysis\n",
    "text_posts['full_text'] = text_posts['title'] + ' ' + text_posts['selftext']\n",
    "\n",
    "# Most common words (simple tokenization)\n",
    "# LEARNING: This is basic NLP - we'll use advanced methods later\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Basic word tokenization (lowercase, split on spaces)\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    # Convert to lowercase and split\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return words\n",
    "\n",
    "# Collect all words\n",
    "all_words = []\n",
    "for text in text_posts['full_text']:\n",
    "    all_words.extend(simple_tokenize(text))\n",
    "\n",
    "# Count frequencies\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Remove common English stop words (basic list)\n",
    "stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', \n",
    "              'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'been', 'be',\n",
    "              'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "              'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those',\n",
    "              'i', 'you', 'he', 'she', 'it', 'we', 'they', 'my', 'your', 'his', 'her',\n",
    "              'its', 'our', 'their', 'me', 'him', 'us', 'them', 'im', 've', 't', 's'}\n",
    "\n",
    "filtered_freq = {word: count for word, count in word_freq.items() \n",
    "                 if word not in stop_words and len(word) > 2}\n",
    "\n",
    "# Top 30 most common words\n",
    "top_words = Counter(filtered_freq).most_common(30)\n",
    "\n",
    "# Visualize\n",
    "words, counts = zip(*top_words)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.barh(words, counts)\n",
    "ax.set_title('Top 30 Most Common Words (excluding stop words)', fontweight='bold')\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mental health keyword frequency\n",
    "# LEARNING: Domain-specific keyword analysis\n",
    "mental_health_terms = {\n",
    "    'depression': ['depression', 'depressed', 'depressing'],\n",
    "    'anxiety': ['anxiety', 'anxious', 'panic'],\n",
    "    'mood': ['mood', 'moods', 'emotional', 'emotions'],\n",
    "    'anger': ['anger', 'angry', 'rage', 'irritable', 'irritability'],\n",
    "    'crying': ['crying', 'cry', 'cried', 'tears'],\n",
    "    'suicidal': ['suicidal', 'suicide', 'kill myself'],\n",
    "    'mental_health': ['mental health', 'mentally']\n",
    "}\n",
    "\n",
    "def count_term_occurrences(text, terms):\n",
    "    \"\"\"Count how many times any variant of a term appears\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    text_lower = text.lower()\n",
    "    return sum(text_lower.count(term) for term in terms)\n",
    "\n",
    "# Count occurrences for each category\n",
    "term_counts = {}\n",
    "for category, terms in mental_health_terms.items():\n",
    "    term_counts[category] = text_posts['full_text'].apply(\n",
    "        lambda x: count_term_occurrences(x, terms)\n",
    "    ).sum()\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "categories = list(term_counts.keys())\n",
    "counts = list(term_counts.values())\n",
    "ax.bar(categories, counts)\n",
    "ax.set_title('Mental Health Term Frequencies', fontweight='bold')\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Total Occurrences')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMental Health Term Counts:\")\n",
    "for category, count in sorted(term_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {category}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample Posts Review\n",
    "\n",
    "Let's look at actual examples to understand the data qualitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few example posts\n",
    "print(\"Sample Posts (high engagement):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get top posts by score\n",
    "top_posts = text_posts.nlargest(3, 'score')\n",
    "\n",
    "for idx, post in top_posts.iterrows():\n",
    "    print(f\"\\n[{post['subreddit']}] Score: {post['score']}\")\n",
    "    print(f\"Title: {post['title']}\")\n",
    "    print(f\"Text: {post['selftext'][:300]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "### What We Learned:\n",
    "1. **Data Quality**: Checked for missing values, duplicates\n",
    "2. **Distributions**: Understood text length, engagement patterns\n",
    "3. **Content**: Identified key mental health terms\n",
    "4. **Temporal**: Saw when posts were created\n",
    "\n",
    "### Next Steps for Your Learning Journey:\n",
    "1. **Text Preprocessing**: Clean and normalize text\n",
    "2. **Named Entity Recognition (NER)**: Extract drug names, symptoms\n",
    "3. **LLM-based Extraction**: Use Claude/GPT to extract structured data\n",
    "4. **Sentiment Analysis**: Analyze emotional tone\n",
    "5. **Knowledge Graph**: Build relationships between entities\n",
    "\n",
    "### Data Quality Assessment:\n",
    "- Are there enough posts for analysis?\n",
    "- Is the data representative?\n",
    "- Do we need more data from specific subreddits?\n",
    "- What preprocessing is needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary statistics\n",
    "summary = {\n",
    "    'total_posts': len(df),\n",
    "    'text_posts': len(text_posts),\n",
    "    'subreddits': text_posts['subreddit'].value_counts().to_dict(),\n",
    "    'avg_text_length': text_posts['text_length'].mean(),\n",
    "    'avg_score': text_posts['score'].mean(),\n",
    "    'date_range': {\n",
    "        'earliest': text_posts['created_datetime'].min().isoformat(),\n",
    "        'latest': text_posts['created_datetime'].max().isoformat()\n",
    "    },\n",
    "    'mental_health_terms': term_counts\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "output_path = Path('../outputs/reports/eda_summary.json')\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Summary saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
