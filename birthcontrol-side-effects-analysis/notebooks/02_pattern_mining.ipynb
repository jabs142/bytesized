{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Mining: Discovering Hidden Symptom Relationships ðŸ”¬\n",
    "\n",
    "**Goal**: Find co-occurring symptoms that clinical trials miss\n",
    "\n",
    "**Learning Concepts**:\n",
    "- Association Rule Mining (Apriori algorithm)\n",
    "- Support, Confidence, and Lift metrics\n",
    "- Pattern discovery in real-world data\n",
    "- Temporal pattern extraction\n",
    "\n",
    "**Example Discoveries**:\n",
    "- \"yeast_infection + vaginal_dryness + long_term_use\" (Support: 45 posts, Confidence: 78%)\n",
    "- \"post_pill + acne + hair_loss\" (Support: 32 posts, Confidence: 65%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport sys\nsys.path.append('../src')\n\nimport json\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom collections import Counter\nfrom tqdm.notebook import tqdm  # Progress bars for Jupyter\n\n# Our custom analyzers\nfrom analysis.medical_term_extractor import MedicalTermExtractor\nfrom analyzers.association_rules import AssociationRulesMiner\n\n# Styling\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\nprint(\"âœ“ Libraries loaded!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data\n",
    "\n",
    "We collected 537 posts from 4 subreddits with expanded symptom keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the newest data file\n",
    "data_path = Path('../data/raw/reddit_bc_symptoms_posts_20251027_175721.json')\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    posts = json.load(f)\n",
    "\n",
    "print(f\"ðŸ“Š Loaded {len(posts)} posts\")\n",
    "print(f\"   Subreddits: {set(p['subreddit'] for p in posts)}\")\n",
    "print(f\"   Total text: {sum(p['text_length'] for p in posts):,} characters\")\n",
    "print(f\"   Average score: {sum(p['score'] for p in posts) / len(posts):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(posts)\n",
    "df['created_date'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "\n",
    "# Combine title and selftext for full analysis\n",
    "df['full_text'] = df['title'] + ' ' + df['selftext']\n",
    "\n",
    "print(\"\\nðŸ“ˆ Data Overview:\")\n",
    "print(df[['subreddit', 'score', 'num_comments', 'text_length']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Symptoms\n",
    "\n",
    "Use our expanded `MedicalTermExtractor` to identify:\n",
    "- **Mental symptoms**: depression, anxiety, mood swings, brain fog, etc.\n",
    "- **Physical symptoms**: acne, yeast infections, dryness, hair loss, etc.\n",
    "- **Temporal markers**: long-term use, just started, stopped, post-pill\n",
    "- **User context**: first-time user, switcher, long-term user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize extractor\nextractor = MedicalTermExtractor()\n\nprint(\"ðŸ”¬ Extracting symptoms from all posts...\")\nprint(\"   Watch the progress bar below!\\n\")\n\n# Extract symptoms for each post\nanalyzed_posts = []\n\nfor post in tqdm(posts, desc=\"Extracting symptoms\", unit=\"post\"):\n    full_text = post['title'] + ' ' + post['selftext']\n    \n    analysis = {\n        'post_id': post['id'],\n        'subreddit': post['subreddit'],\n        'score': post['score'],\n        'created_date': post['created_date'],\n        \n        # Extract all symptoms\n        'symptoms': extractor.extract_symptoms(full_text, category='all'),\n        'mental_symptoms': extractor.extract_symptoms(full_text, category='mental'),\n        'physical_symptoms': extractor.extract_symptoms(full_text, category='physical'),\n        \n        # Extract temporal and user context\n        'temporal_context': extractor.extract_temporal_context(full_text),\n        'user_context': extractor.extract_user_context(full_text),\n        \n        # Extract birth control types\n        'bc_types': extractor.extract_bc_types(full_text),\n    }\n    \n    analyzed_posts.append(analysis)\n\nprint(f\"\\nâœ“ Extracted symptoms from {len(analyzed_posts)} posts!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "posts_with_symptoms = [p for p in analyzed_posts if len(p['symptoms']) > 0]\n",
    "mental_only = [p for p in analyzed_posts if len(p['mental_symptoms']) > 0 and len(p['physical_symptoms']) == 0]\n",
    "physical_only = [p for p in analyzed_posts if len(p['physical_symptoms']) > 0 and len(p['mental_symptoms']) == 0]\n",
    "both = [p for p in analyzed_posts if len(p['mental_symptoms']) > 0 and len(p['physical_symptoms']) > 0]\n",
    "\n",
    "print(\"\\nðŸ“Š Symptom Statistics:\")\n",
    "print(f\"   Posts with symptoms: {len(posts_with_symptoms)} ({len(posts_with_symptoms)/len(analyzed_posts)*100:.1f}%)\")\n",
    "print(f\"   Mental symptoms only: {len(mental_only)} posts\")\n",
    "print(f\"   Physical symptoms only: {len(physical_only)} posts\")\n",
    "print(f\"   Both mental + physical: {len(both)} posts\")\n",
    "print(f\"\\n   Average symptoms per post: {np.mean([len(p['symptoms']) for p in posts_with_symptoms]):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common symptoms overall\n",
    "all_symptoms = Counter()\n",
    "for post in analyzed_posts:\n",
    "    for symptom in post['symptoms'].keys():\n",
    "        all_symptoms[symptom] += 1\n",
    "\n",
    "print(\"\\nðŸ” Top 20 Most Common Symptoms:\")\n",
    "for symptom, count in all_symptoms.most_common(20):\n",
    "    pct = (count / len(posts_with_symptoms)) * 100\n",
    "    print(f\"   {symptom:25} {count:4} posts ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Association Rule Mining\n",
    "\n",
    "**Goal**: Find symptom combinations that appear together more often than random chance.\n",
    "\n",
    "**Metrics**:\n",
    "- **Support**: How many posts have this pattern? (minimum: 3% = ~16 posts)\n",
    "- **Confidence**: If symptom A appears, what % also have symptom B? (minimum: 60%)\n",
    "- **Lift**: How much stronger is this pattern than random? (minimum: 1.2x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pattern miner with thresholds\n",
    "miner = AssociationRulesMiner(\n",
    "    min_support=int(len(posts_with_symptoms) * 0.03),  # 3% of posts = ~16 posts\n",
    "    min_confidence=0.60,  # 60% confidence\n",
    "    min_lift=1.2  # 20% better than random\n",
    ")\n",
    "\n",
    "print(f\"ðŸ” Mining patterns with:\")\n",
    "print(f\"   Min support: {miner.min_support} posts ({miner.min_support/len(posts_with_symptoms)*100:.1f}%)\")\n",
    "print(f\"   Min confidence: {miner.min_confidence:.0%}\")\n",
    "print(f\"   Min lift: {miner.min_lift}x\")\n",
    "print(\"\\n   This will take 1-2 minutes...\\n\")\n",
    "\n",
    "# Run the Apriori algorithm!\n",
    "rules = miner.find_patterns(analyzed_posts)\n",
    "\n",
    "print(f\"\\nâœ“ Found {len(rules)} association rules!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Discovered Patterns\n",
    "\n",
    "Let's explore the strongest symptom relationships!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rules to DataFrame for easier analysis\n",
    "rules_df = pd.DataFrame(rules)\n",
    "\n",
    "if len(rules_df) > 0:\n",
    "    # Format for display\n",
    "    rules_df['antecedent_str'] = rules_df['antecedent'].apply(lambda x: ' + '.join(x))\n",
    "    rules_df['consequent_str'] = rules_df['consequent'].apply(lambda x: ' + '.join(x))\n",
    "    rules_df['rule'] = rules_df['antecedent_str'] + ' â†’ ' + rules_df['consequent_str']\n",
    "    \n",
    "    print(\"\\nðŸ† Top 20 Strongest Patterns (by Lift):\\n\")\n",
    "    for idx, rule in rules_df.head(20).iterrows():\n",
    "        print(f\"{rule['antecedent_str']:40} â†’ {rule['consequent_str']:30}\")\n",
    "        print(f\"   Support: {rule['support']:3} posts ({rule['support_pct']:5.1f}%)\")\n",
    "        print(f\"   Confidence: {rule['confidence']:5.1%}  |  Lift: {rule['lift']:.2f}x\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"âš ï¸  No rules found matching the thresholds. Try lowering min_support or min_confidence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rule distribution\n",
    "if len(rules_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Support distribution\n",
    "    axes[0].hist(rules_df['support'], bins=20, edgecolor='black')\n",
    "    axes[0].set_xlabel('Support (number of posts)')\n",
    "    axes[0].set_ylabel('Number of Rules')\n",
    "    axes[0].set_title('Support Distribution')\n",
    "    axes[0].axvline(miner.min_support, color='red', linestyle='--', label=f'Min: {miner.min_support}')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Confidence distribution\n",
    "    axes[1].hist(rules_df['confidence'], bins=20, edgecolor='black')\n",
    "    axes[1].set_xlabel('Confidence')\n",
    "    axes[1].set_ylabel('Number of Rules')\n",
    "    axes[1].set_title('Confidence Distribution')\n",
    "    axes[1].axvline(miner.min_confidence, color='red', linestyle='--', label=f'Min: {miner.min_confidence:.0%}')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Lift distribution\n",
    "    axes[2].hist(rules_df['lift'], bins=20, edgecolor='black')\n",
    "    axes[2].set_xlabel('Lift')\n",
    "    axes[2].set_ylabel('Number of Rules')\n",
    "    axes[2].set_title('Lift Distribution')\n",
    "    axes[2].axvline(miner.min_lift, color='red', linestyle='--', label=f'Min: {miner.min_lift}x')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ“Š Rule Statistics:\")\n",
    "    print(f\"   Average support: {rules_df['support'].mean():.1f} posts\")\n",
    "    print(f\"   Average confidence: {rules_df['confidence'].mean():.1%}\")\n",
    "    print(f\"   Average lift: {rules_df['lift'].mean():.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Specific Patterns\n",
    "\n",
    "Let's look at specific types of patterns:\n",
    "- Mental + Physical co-occurrence\n",
    "- Post-pill patterns\n",
    "- Long-term use patterns\n",
    "- Birth control type-specific patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to categorize symptoms\n",
    "mental_symptoms = ['depression', 'anxiety', 'mood_swings', 'brain_fog', 'panic_attacks', \n",
    "                   'emotional_numbness', 'suicidal_thoughts', 'low_mood', 'irritability']\n",
    "\n",
    "physical_symptoms = ['acne', 'yeast_infection', 'vaginal_dryness', 'hair_loss', 'weight_gain',\n",
    "                     'bloating', 'low_libido', 'heavy_bleeding', 'spotting', 'cramping',\n",
    "                     'headache', 'migraine', 'nausea']\n",
    "\n",
    "def is_mental(symptom):\n",
    "    return symptom in mental_symptoms\n",
    "\n",
    "def is_physical(symptom):\n",
    "    return symptom in physical_symptoms\n",
    "\n",
    "# Find mental â†’ physical rules\n",
    "if len(rules_df) > 0:\n",
    "    mental_physical_rules = rules_df[\n",
    "        rules_df['antecedent'].apply(lambda x: any(is_mental(s) for s in x)) &\n",
    "        rules_df['consequent'].apply(lambda x: any(is_physical(s) for s in x))\n",
    "    ].sort_values('lift', ascending=False)\n",
    "    \n",
    "    print(\"\\nðŸ§ âž¡ï¸ðŸ’Š Mental â†’ Physical Symptom Patterns:\\n\")\n",
    "    if len(mental_physical_rules) > 0:\n",
    "        for idx, rule in mental_physical_rules.head(10).iterrows():\n",
    "            print(f\"{rule['antecedent_str']:35} â†’ {rule['consequent_str']:25}\")\n",
    "            print(f\"   Support: {rule['support']} posts | Confidence: {rule['confidence']:.1%} | Lift: {rule['lift']:.2f}x\\n\")\n",
    "    else:\n",
    "        print(\"   No mentalâ†’physical patterns found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal pattern analysis\n",
    "temporal_stats = Counter()\n",
    "for post in analyzed_posts:\n",
    "    for marker in post['temporal_context']:\n",
    "        temporal_stats[marker] += 1\n",
    "\n",
    "print(\"\\nâ° Temporal Patterns:\")\n",
    "for marker, count in temporal_stats.most_common():\n",
    "    pct = (count / len(analyzed_posts)) * 100\n",
    "    print(f\"   {marker:20} {count:4} posts ({pct:5.1f}%)\")\n",
    "\n",
    "# User context analysis\n",
    "context_stats = Counter()\n",
    "for post in analyzed_posts:\n",
    "    for ctx in post['user_context']:\n",
    "        context_stats[ctx] += 1\n",
    "\n",
    "print(\"\\nðŸ‘¤ User Context:\")\n",
    "for ctx, count in context_stats.most_common():\n",
    "    pct = (count / len(analyzed_posts)) * 100\n",
    "    print(f\"   {ctx:20} {count:4} posts ({pct:5.1f}%)\")\n",
    "\n",
    "# Birth control type analysis\n",
    "bc_stats = Counter()\n",
    "for post in analyzed_posts:\n",
    "    for bc in post['bc_types']:\n",
    "        bc_stats[bc] += 1\n",
    "\n",
    "print(\"\\nðŸ’Š Birth Control Types Mentioned:\")\n",
    "for bc, count in bc_stats.most_common():\n",
    "    pct = (count / len(analyzed_posts)) * 100\n",
    "    print(f\"   {bc:25} {count:4} posts ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Export for Web Visualization\n",
    "\n",
    "Export the discovered patterns in JSON format for the interactive web app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('../data/patterns')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export network graph data for visualization\n",
    "if len(rules) > 0:\n",
    "    graph_output = output_dir / 'symptom_network.json'\n",
    "    miner.export_for_visualization(rules, str(graph_output))\n",
    "    print(f\"âœ“ Exported network graph to {graph_output}\")\n",
    "\n",
    "# Export top patterns\n",
    "patterns_output = output_dir / 'discovered_patterns.json'\n",
    "if len(rules_df) > 0:\n",
    "    top_patterns = rules_df.head(50)[[\n",
    "        'antecedent', 'consequent', 'support', 'confidence', 'lift', 'support_pct'\n",
    "    ]].to_dict('records')\n",
    "    \n",
    "    with open(patterns_output, 'w') as f:\n",
    "        json.dump(top_patterns, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ“ Exported top 50 patterns to {patterns_output}\")\n",
    "\n",
    "# Export summary statistics\n",
    "stats_output = output_dir / 'stats.json'\n",
    "stats = {\n",
    "    'total_posts': len(posts),\n",
    "    'posts_with_symptoms': len(posts_with_symptoms),\n",
    "    'total_rules_found': len(rules),\n",
    "    'mining_params': {\n",
    "        'min_support': miner.min_support,\n",
    "        'min_confidence': miner.min_confidence,\n",
    "        'min_lift': miner.min_lift\n",
    "    },\n",
    "    'top_symptoms': dict(all_symptoms.most_common(20)),\n",
    "    'temporal_patterns': dict(temporal_stats),\n",
    "    'user_contexts': dict(context_stats),\n",
    "    'bc_types': dict(bc_stats),\n",
    "    'symptom_categories': {\n",
    "        'mental_only': len(mental_only),\n",
    "        'physical_only': len(physical_only),\n",
    "        'both': len(both)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(stats_output, 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Exported statistics to {stats_output}\")\n",
    "print(\"\\nâœ… All pattern mining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "**What We Discovered**:\n",
    "- Found hidden symptom co-occurrence patterns using association rule mining\n",
    "- Identified mental + physical symptom relationships\n",
    "- Extracted temporal patterns (long-term use, post-pill, etc.)\n",
    "- Analyzed user contexts and birth control types\n",
    "\n",
    "**Key Learnings**:\n",
    "- **Support**: How common is a pattern?\n",
    "- **Confidence**: If A appears, what % also have B?\n",
    "- **Lift**: Is the pattern stronger than random chance?\n",
    "- **Apriori Algorithm**: Efficiently builds patterns incrementally\n",
    "\n",
    "**Next Steps**:\n",
    "1. âœ… Pattern mining complete â†’ Ready for visualization!\n",
    "2. ðŸ”œ Build temporal patterns analyzer (timeline extraction)\n",
    "3. ðŸ”œ Build network analysis module (community detection)\n",
    "4. ðŸ”œ Create mobile-friendly web app with interactive visualizations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (BC Analysis)",
   "language": "python",
   "name": "birthcontrol-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}